{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "865b1079-f123-425c-a83d-875aff9feb8a",
   "metadata": {},
   "source": [
    "# Easy ways to improve the search relevancy\n",
    "\n",
    "Up till now, we have tested various embedding models, but it doesn't mean we have covered all the possible ways to make sure our search results are relevant. Relevance is sometimes not about the document itself, but about some additional criteria, such as geographical proximity, recency, or some other business rules that none of the models can capture. They may also vary over time, so it's clear we cannot encode them directly in the vectors. Qdrant has some mechanisms that can boost the quality of the retrieval outputs with little effort, so no vector computations are required."
   ]
  },
  {
   "cell_type": "code",
   "id": "2223b80a-a97c-4a0e-b50d-6af8dc7605bc",
   "metadata": {},
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3b87ea47-ac5e-4831-83f4-ec29eaf0740c",
   "metadata": {},
   "source": [
    "# See: https://qdrant.github.io/fastembed/examples/Supported_Models/#supported-text-embedding-models\n",
    "COLLECTION_NAME = \"hackernews-hybrid-rag\"\n",
    "\n",
    "# Dense retrieval\n",
    "MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
    "VECTOR_SIZE = 384\n",
    "VECTOR_NAME = \"bge-small-en-v1.5\"\n",
    "\n",
    "# Sparse model\n",
    "BM25_MODEL_NAME = \"Qdrant/bm25\"\n",
    "BM25_VECTOR_NAME = \"bm25\"\n",
    "\n",
    "# Token-level representations\n",
    "MUTLIVECTOR_MODEL_NAME = \"colbert-ir/colbertv2.0\"\n",
    "MULTIVECTOR_SIZE = 128\n",
    "MULTIVECTOR_NAME = \"colbertv2.0\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5a16948-54ae-4772-a4e0-eef7e7c19af2",
   "metadata": {},
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "import os\n",
    "\n",
    "client = QdrantClient(\n",
    "    os.environ.get(\"QDRANT_URL\"), \n",
    "    api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a51e3942-a344-4003-b2d4-fdd77138a344",
   "metadata": {},
   "source": [
    "from any_llm import acompletion\n",
    "from typing import Callable\n",
    "\n",
    "RetieverFunc = Callable[[str, int], list[str]]\n",
    "\n",
    "LLM_NAME = \"claude-sonnet-4-20250514\"\n",
    "\n",
    "async def rag(q: str, retrieve_func: RetieverFunc, *, n_docs: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Run single-turn RAG on a given input query.\n",
    "    Return just the model response.\n",
    "    \"\"\"\n",
    "    docs = retrieve_func(q, n_docs)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Please provide a response to my question based only \" +\n",
    "                \"on the provided context and only it. If it doesn't \" +\n",
    "                \"contain any helpful information, please let me know \" +\n",
    "                \"and admit you cannot produce relevant answer.\\n\" +\n",
    "                f\"<context>{'\\n'.join(docs)}</context>\\n\" +\n",
    "                f\"<question>{q}</question>\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    response = await acompletion(\n",
    "        provider=os.environ.get(\"LLM_PROVIDER\"),\n",
    "        model=LLM_NAME,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fce63321-5bb5-41f0-be6a-a4166fc2d538",
   "metadata": {},
   "source": [
    "## Search diversity\n",
    "\n",
    "Your Retrieval Augmented Generation might be only as good as the retrieved documents provided to the LLM. A common issue in RAG-like applications is a lack of diversity in the retrieved documents and passing dozens of near duplicates. If a document does not bring any additional information, then we're only wasting tokens. It makes sense to diversify the set of results to cover a broader spectrum. Unfortunately, vector search alone will always return the documents with the highest scores possible, and that's what it is expected to do. Search results diversification is typically achieved as a post-processing step, and for that, we need to retrieve more candidates and choose a subset that maximizes the diversity. Qdrant has implemented a Maximal Marginal Relevance algorithm that does exactly this. It's also part of the Universal Query API."
   ]
  },
  {
   "cell_type": "code",
   "id": "a71297e4-220b-46cd-9dd7-d0398068bbce",
   "metadata": {},
   "source": [
    "def retrieve_diverse(q: str, n_docs: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve documents based on the provided query\n",
    "    with BM25 and dense retrieval + MMR to diversify\n",
    "    on ColBERT vectors.\n",
    "    \"\"\"\n",
    "    result = client.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=q,\n",
    "                    model=BM25_MODEL_NAME,\n",
    "                ),\n",
    "                using=BM25_VECTOR_NAME,\n",
    "                # Ten times more than expected\n",
    "                limit=(n_docs * 10),\n",
    "            ),\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=q,\n",
    "                    model=MODEL_NAME,\n",
    "                ),\n",
    "                using=VECTOR_NAME,\n",
    "                # Ten times more than expected\n",
    "                limit=(n_docs * 10),\n",
    "            ),\n",
    "        ],\n",
    "        # Maximal Marginal Relevance\n",
    "        query=models.NearestQuery(\n",
    "            nearest=models.Document(\n",
    "                text=q,\n",
    "                model=MUTLIVECTOR_MODEL_NAME,\n",
    "            ),\n",
    "            mmr=models.Mmr(\n",
    "                # 0.0 - relevance only, 1.0 - diversity only\n",
    "                diversity=0.75,\n",
    "            )\n",
    "        ),\n",
    "        using=MULTIVECTOR_NAME,\n",
    "        limit=n_docs,\n",
    "    )\n",
    "    docs = [\n",
    "        f\"{point.payload['datetime']} {point.payload['title']} {point.payload['text']}\"\n",
    "        for point in result.points\n",
    "    ]\n",
    "    return docs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "42adf477-4243-41de-b008-420d2ba4b97d",
   "metadata": {},
   "source": [
    "retrieve_diverse(\"How do I perform a KNN search on a large scale?\", n_docs=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0d42ed3-453b-4d8c-9bd8-869b9d0f2d77",
   "metadata": {},
   "source": [
    "response = await rag(\n",
    "    \"How do I perform a KNN search on a large scale?\", \n",
    "    retrieve_func=retrieve_diverse\n",
    ")\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "777b07f6-93a4-4cbf-a8ee-0c5d36422484",
   "metadata": {},
   "source": [
    "response = await rag(\n",
    "    \"What is the community working on?\", \n",
    "    retrieve_func=retrieve_diverse\n",
    ")\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "04d7e404-54ed-435f-acfc-02f593d64bad",
   "metadata": {},
   "source": [
    "retrieve_diverse(\"What is the community working on?\", n_docs=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d3c2ae3c-c66d-4d3b-9c84-d06b9626989f",
   "metadata": {},
   "source": [
    "## Applying business rules\n",
    "\n",
    "Search does not cover factors such as geographical proximity or recency. We could theoretically apply payload filters to restrict data coming from last week, month, or year, but it's not an ideal solution if we want to express a preference, not a hard limit. If we want to combine relevance with some additional criteria, we need to recalculate the scores based on the scores as returned by individual methods, and the other factors we want to consider. Score boosting is a mechanism providing a way to achieve exactly this.\n",
    "\n",
    "HackerNews provides a `time` attribute we converted to a proper `datetime` at the very beginning. Let's try to use if for recency."
   ]
  },
  {
   "cell_type": "code",
   "id": "24321594-5308-4e5e-9c31-385cb3036a16",
   "metadata": {},
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "def retrieve_recent(q: str, n_docs: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve documents based on the provided query\n",
    "    with BM25 and dense retrieval + recency.\n",
    "    \"\"\"\n",
    "    result = client.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=q,\n",
    "                    model=BM25_MODEL_NAME,\n",
    "                ),\n",
    "                using=BM25_VECTOR_NAME,\n",
    "                # Ten times more than expected\n",
    "                limit=(n_docs * 10),\n",
    "            ),\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=q,\n",
    "                    model=MODEL_NAME,\n",
    "                ),\n",
    "                using=VECTOR_NAME,\n",
    "                # Ten times more than expected\n",
    "                limit=(n_docs * 10),\n",
    "            ),\n",
    "        ],\n",
    "        # Score boosting\n",
    "        query=models.FormulaQuery(\n",
    "            formula=models.MultExpression(\n",
    "                mult=[\n",
    "                    \"$score\",\n",
    "                    models.ExpDecayExpression(\n",
    "                        exp_decay=models.DecayParamsExpression(\n",
    "                            x=models.DatetimeKeyExpression(\n",
    "                                datetime_key=\"datetime\" # payload key \n",
    "                            ),\n",
    "                            # Current datetime in \"2025-09-25T00:00:00Z format\n",
    "                            target=models.DatetimeExpression(\n",
    "                                datetime=datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "                            ),\n",
    "                            scale=86400 * 365, # 1 day in seconds * 365\n",
    "                            # If item's \"datetime\" is more than 1 year apart from \n",
    "                            # the current datetime, relevance score is less than 0.5\n",
    "                            midpoint=0.5\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        # Five times more than expected too\n",
    "        limit=n_docs,\n",
    "    )\n",
    "    docs = [\n",
    "        f\"{point.payload['datetime']} {point.payload['title']} {point.payload['text']}\"\n",
    "        for point in result.points\n",
    "    ]\n",
    "    return docs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5542664c-28fe-470d-a6e8-169365522986",
   "metadata": {},
   "source": [
    "retrieve_recent(\"What is the community working on?\", n_docs=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98359de9-9928-45b5-aece-20471fb56dd6",
   "metadata": {},
   "source": [
    "response = await rag(\n",
    "    \"What is the community working on?\", \n",
    "    retrieve_func=retrieve_recent\n",
    ")\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "898b5a92-7c53-4740-9a68-e6369549ffc1",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
