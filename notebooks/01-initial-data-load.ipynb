{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f912613b-cbff-4c74-900b-5a198e5bd501",
   "metadata": {},
   "source": [
    "# Filling a collection with data\n",
    "\n",
    "We'll start our experiments with a collection filled with [HackerNews](https://news.ycombinator.com/) submissions. Retrieval Augmented Generation is typically built with dense vectors, so let's try if it works in all the cases we would like to support. The [hackernews.csv](../data/hackernews.csv) is a dump of HN submissions, without the comments. Let's process it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad45c70-f24b-4b24-8d6b-0c16c5411507",
   "metadata": {},
   "source": [
    "## Setting up Qdrant collection\n",
    "\n",
    "Our collection needs to be configured for a single vector per point. Even though we have just a single vector, we'll use named vectors. If you want to use a different model, it's the time to configure it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc6026b-976b-41b0-a473-748b0a5be0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eadb5bd9-0b33-4634-8dc3-978418ef8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://qdrant.github.io/fastembed/examples/Supported_Models/#supported-text-embedding-models\n",
    "COLLECTION_NAME = \"hackernews-rag\"\n",
    "MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
    "VECTOR_SIZE = 384\n",
    "VECTOR_NAME = \"bge-small-en-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b60a7349-af98-4ff2-a197-d24bca44ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "import os\n",
    "\n",
    "client = QdrantClient(\n",
    "    os.environ.get(\"QDRANT_URL\"), \n",
    "    api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dde8961c-797b-4a5d-affe-a9fad9e1b2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config={\n",
    "        VECTOR_NAME: models.VectorParams(\n",
    "            size=VECTOR_SIZE,\n",
    "            distance=models.Distance.COSINE,\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f3808e-6f4b-442f-a6e6-ae63ae9fc4b6",
   "metadata": {},
   "source": [
    "## Processing input data\n",
    "\n",
    "Our dataset is a regular CSV file we need to iterate over and store in Qdrant. We'll use a local inference mode based on Qdrant<>FastEmbed integration, so we don't need to compute the vectors separately, but can just pass raw data and expect the client to convert it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5525a6c7-0da9-4044-9623-0b38cb8819fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '7811', 'deleted': '', 'type': 'story', 'by': 'Oldude59', 'time': '1175345257', 'text': '', 'dead': '', 'parent': '', 'poll': '', 'url': 'http://buscreate.blogspot.com', 'score': '1', 'title': 'Business Creativity and Happiness', 'descendants': '0', 'karma': '2'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"../data/hackernews.csv\", newline=\"\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    row = next(reader)\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d58bf1cc-15b2-4087-8ccf-29333a296c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10405it [00:00, 15508.45it/s]\n"
     ]
    }
   ],
   "source": [
    "from itertools import batched\n",
    "from tqdm import tqdm\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "\n",
    "with open(\"../data/hackernews.csv\", newline=\"\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for batch in tqdm(batched(reader, n=16)):\n",
    "        client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[\n",
    "                models.PointStruct(\n",
    "                    # HackerNews id is Qdrant id as well\n",
    "                    id=int(point[\"id\"]),\n",
    "                    vector={\n",
    "                        VECTOR_NAME: models.Document(\n",
    "                            text=f\"{point['title']} {point['text']}\",\n",
    "                            model=MODEL_NAME,\n",
    "                        )\n",
    "                    },\n",
    "                    payload={\n",
    "                        \"datetime\": datetime.utcfromtimestamp(int(point[\"time\"])).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "                        **point\n",
    "                    },\n",
    "                )\n",
    "                for point in batch\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e98b207-a6a5-4452-9856-16e0ce51d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.recover_snapshot(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    # Please do not modify the URL below\n",
    "    location=\"https://storage.googleapis.com/tutorials-snapshots-bucket/workshop-improving-r-in-rag/hackernews-rag.snapshot\",\n",
    "    wait=False, # Loading a snapshot may take some time, so let's avoid a timeout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc7663-14bc-4268-ae68-fc26cf05b6b3",
   "metadata": {},
   "source": [
    "## Building RAG with Qdrant-based retrieval\n",
    "\n",
    "Let's build a naive RAG with dense vector search. It'll be a very basic process, using the original prompt as a query and then passes retrieved context to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f38b3d9-b6cd-4875-a94e-26526e16f1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='claude-opus-4-1-20250805', created=1754352000, object='model', owned_by='anthropic'),\n",
       " Model(id='claude-opus-4-20250514', created=1747872000, object='model', owned_by='anthropic'),\n",
       " Model(id='claude-sonnet-4-20250514', created=1747872000, object='model', owned_by='anthropic'),\n",
       " Model(id='claude-3-7-sonnet-20250219', created=1740355200, object='model', owned_by='anthropic'),\n",
       " Model(id='claude-3-5-sonnet-20241022', created=1729555200, object='model', owned_by='anthropic'),\n",
       " Model(id='claude-3-5-haiku-20241022', created=1729555200, object='model', owned_by='anthropic'),\n",
       " Model(id='claude-3-5-sonnet-20240620', created=1718841600, object='model', owned_by='anthropic'),\n",
       " Model(id='claude-3-haiku-20240307', created=1709769600, object='model', owned_by='anthropic'),\n",
       " Model(id='claude-3-opus-20240229', created=1709164800, object='model', owned_by='anthropic')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from any_llm import list_models\n",
    "\n",
    "list_models(provider=os.environ.get(\"LLM_PROVIDER\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b19a9a4b-34ff-4bc5-ac75-06b10863dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(q: str, n_docs: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve documents based on the provided query\n",
    "    \"\"\"\n",
    "    result = client.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query=models.Document(\n",
    "            text=q,\n",
    "            model=MODEL_NAME,\n",
    "        ),\n",
    "        using=VECTOR_NAME,\n",
    "        limit=n_docs,\n",
    "    )\n",
    "    docs = [\n",
    "        f\"{point.payload['title']} {point.payload['text']}\"\n",
    "        for point in result.points\n",
    "    ]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47ebe9f7-59a5-412f-92bc-63ba94359df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9110b931ffc743609f94667e14a89b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46574de8dfd64b0c8921495c29fd04a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff8b52634f54dbda9fc9fe61e26269c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c99f72c63b4f05ab067372faa1bba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/706 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a692b33c824e508b8b79bc2f2e2567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bb79a4d0b34ab7b4f77bad490e9d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_optimized.onnx:   0%|          | 0.00/66.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Why to build an AI startup ',\n",
       " 'Startup Ideas ',\n",
       " 'Discovering Cool Ideas Making Money with AI ',\n",
       " 'Some startup ideas ',\n",
       " 'Where are the opportunities for new startups in generative AI? ',\n",
       " 'Make any AI generated app ',\n",
       " 'Billion Dollar Startup Ideas ',\n",
       " 'AI Startup Wants to Create Jobs, Not Take Them Away ',\n",
       " 'Explore Side Hustle Ideas to Make Money with AI ',\n",
       " 'Top Generative AI Startups 2024 ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve(\"What are the coolest ideas for an AI startup?\", n_docs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c5a0d8-20b6-402b-8de8-824172056ecd",
   "metadata": {},
   "source": [
    "### Payload-based filtering\n",
    "\n",
    "HackeNews submissions may have just a title, but in such a case they rarely provide any useful information. It seems to make sense to exclude such submissions entirely, and only focus on the ones having some more details than just the submission title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77e4aece-4cec-4e17-ae64-092043ac6f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=10416, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_payload_index(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    field_name=\"text\",\n",
    "    field_schema=\"keyword\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76c2a110-ef99-42b5-b2d6-8573352ae175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_filtered(q: str, n_docs: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve documents based on the provided query,\n",
    "    but only those which have non-empty text attribute.\n",
    "    \"\"\"\n",
    "    result = client.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query=models.Document(\n",
    "            text=q,\n",
    "            model=MODEL_NAME,\n",
    "        ),\n",
    "        query_filter=models.Filter(\n",
    "            must_not=[\n",
    "                # Lack of field\n",
    "                models.IsEmptyCondition(\n",
    "                    is_empty=models.PayloadField(key=\"text\"),\n",
    "                ),\n",
    "                # Field set to null value\n",
    "                models.IsNullCondition(\n",
    "                    is_null=models.PayloadField(key=\"text\"),\n",
    "                ),\n",
    "                # Field set to an empty string\n",
    "                models.FieldCondition(\n",
    "                    key=\"text\",\n",
    "                    match=models.MatchValue(value=\"\"),\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "        using=VECTOR_NAME,\n",
    "        limit=n_docs,\n",
    "    )\n",
    "    docs = [\n",
    "        f\"{point.payload['title']} {point.payload['text']}\"\n",
    "        for point in result.points\n",
    "    ]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe4be1f8-ee66-4405-9a6b-c36519ab4db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Startup IDEA:- INVEST IN AI MODELS by tokenizing it and selling parts Okay so the idea is similar to ICO(initial coin offering) in which different AI models will be tokenized and funds will be raised.<p>The idea is to create a platform where engineers can raise funds on the models created by them.<p>please review &amp; suggest!',\n",
       " 'What are the productivity improvements you hope AI can help with? I&#x27;m curious to hear about the most ambitious or even the smallest productivity and creativity enhancements you guys believe AI can facilitate',\n",
       " 'Ask HN: Hardware to Build an AI Assistant? Any idea on what to use? My first thought of course is a raspberry pi (I got a lot in a drawer!) but my requirements should be:<p>-a voice activated microphone\\n-an audio output device (speaker)\\n-maybe a single case containing all\\n-optional: a small screen<p>ideas? thanks everyone!',\n",
       " ' AI-generated images look pretty cool sometimes, and I would like authors/publishers to go as crazy possible.',\n",
       " 'Show HN: We are building your AI co-founder to help your run your startup Hey HN,<p>I quit my 9 to 5 to join an accelerator. Let&#x27;s say things didn&#x27;t work out there. After launching my fourth startup and failing miserably again I decided to work on Frederick AI.<p>I realised a lot of the early stage processes from idea to startup are the same and we can use AI to automate a lot of it from the way we come up with an idea to how we run startups.<p>Frederick AI is a set of tools for early stage founders we collect Market Data 24&#x2F;7 to detect consumer, business and government problems for other startup founders to go and solve.<p>We then create a business plan and a landing page to help you get feedback in less than a minute.<p>We are super early still would love to get some feedback from the community!. Let us know if we can build anything for you.',\n",
       " 'Show HN: I build my first micro AI SaaS for icon generator Let’s try generate your unique and beautiful icon by using ai.',\n",
       " \"Ask HN: What's your experience been building AI apps? For those of you building AI apps for your company or for your own product, what&#x27;s your experience been like so far?\",\n",
       " 'Ask HN: Core technology programming product ideas just like llms or system programming \\nand like a balance between technology and busniess logic \\nlike a startup',\n",
       " 'Hiring: Founding Engineer, Founding Designer I am building ColdBook - an AI-driven emailing platform to help startup founders and sales folks.<p>Currently going through ideation and MVP stage.<p>Email: anandu251@gmail.com to get involved.',\n",
       " 'Ask HN: How are you using AI at your company? Not just basic use cases like &quot;ChatGPT&quot;, but maybe deeper applications like Pipeline integration etc.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_filtered(\"What are the coolest ideas for an AI startup?\", n_docs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9eacef8-5970-4364-b124-bcac718917a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from any_llm import acompletion\n",
    "from typing import Callable\n",
    "\n",
    "RetieverFunc = Callable[[str, int], list[str]]\n",
    "\n",
    "\n",
    "async def rag(q: str, retrieve_func: RetieverFunc, *, n_docs: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Run single-turn RAG on a given input query.\n",
    "    Return just the model response.\n",
    "    \"\"\"\n",
    "    docs = retrieve_func(q, n_docs)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Please provide a response to my question based only \" +\n",
    "                \"on the provided context and only it. If it doesn't \" +\n",
    "                \"contain any helpful information, please let me know \" +\n",
    "                \"and admit you cannot produce relevant answer.\\n\" +\n",
    "                f\"<context>{'\\n'.join(docs)}</context>\\n\" +\n",
    "                f\"<question>{q}</question>\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    response = await acompletion(\n",
    "        provider=os.environ.get(\"LLM_PROVIDER\"),\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eaf9be1-9391-4127-8257-219df24079b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/15/25 16:17:08] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> max_tokens is required for Anthropic, setting to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span>                     <a href=\"file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py#279\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">279</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/15/25 16:17:08]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m max_tokens is required for Anthropic, setting to \u001b[1;36m8192\u001b[0m                     \u001b]8;id=818839;file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py\u001b\\\u001b[2mutils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=834434;file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py#279\u001b\\\u001b[2m279\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here are some of the coolest AI startup ideas mentioned:\n",
      "\n",
      "1. **AI Model Tokenization Platform** - A platform similar to ICO (Initial Coin Offering) where AI models are tokenized and engineers can raise funds by selling parts/shares of their AI models to investors.\n",
      "\n",
      "2. **AI Co-founder for Startups (Frederick AI)** - An AI assistant that helps run startups by automating early-stage processes from idea generation to execution. It collects market data 24/7 to detect consumer, business, and government problems, then creates business plans and landing pages in under a minute.\n",
      "\n",
      "3. **AI-Powered Icon Generator** - A micro SaaS that generates unique and beautiful icons using AI technology.\n",
      "\n",
      "4. **AI-Driven Email Platform (ColdBook)** - An emailing platform designed to help startup founders and sales professionals with their outreach efforts.\n",
      "\n",
      "5. **AI Assistant Hardware** - Building physical AI assistant devices with voice activation, audio output, optional screens, potentially using hardware like Raspberry Pi.\n",
      "\n",
      "The context also mentions using AI for generating images for authors/publishers and various pipeline integrations, though these are discussed more generally rather than as specific startup concepts.\n",
      "\n",
      "These ideas span different aspects of AI application - from democratizing AI model investment to automating business processes and creating specialized tools for content generation.\n"
     ]
    }
   ],
   "source": [
    "response = await rag(\n",
    "    \"What are the coolest ideas for an AI startup?\", \n",
    "    retrieve_func=retrieve_filtered\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28bba782-eb93-415c-9c79-a62ec67f479b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/15/25 16:17:16] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> max_tokens is required for Anthropic, setting to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span>                     <a href=\"file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py#279\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">279</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/15/25 16:17:16]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m max_tokens is required for Anthropic, setting to \u001b[1;36m8192\u001b[0m                     \u001b]8;id=918294;file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py\u001b\\\u001b[2mutils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=175279;file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py#279\u001b\\\u001b[2m279\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I can only find limited information about what Qdrant does:\n",
      "\n",
      "1. **Poetry enhancement**: Qdrant can be integrated with GPT-4 to \"elevate its poetry composition capabilities\" and help transform poetry \"with enhanced coherence and depth.\"\n",
      "\n",
      "2. **GPT search functionality**: Qdrant is used as a component in building a search engine for GPTs, specifically mentioned in the context of AssistantHunter, which searches through thousands of GPTs.\n",
      "\n",
      "However, the context doesn't provide a comprehensive explanation of Qdrant's core functionality or what it fundamentally does as a technology platform. The information is limited to these two specific use cases mentioned in passing. I cannot provide a more detailed answer about Qdrant's capabilities based solely on this context.\n"
     ]
    }
   ],
   "source": [
    "response = await rag(\"What does Qdrant do?\", retrieve_func=retrieve_filtered)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6adc233-d8da-4b30-883c-22c28c6ee3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How to Augment GPT-4 with Qdrant to Elevate Its Poetry Composition Capabilities GPT-4 and Qdrant synergize, transforming poetry with enhanced coherence and depth. Visit my medium article to view the code implementation: https:&#x2F;&#x2F;medium.com&#x2F;@akriti.upadhyay&#x2F;how-to-augment-gpt-4-with-qdrant-to-elevate-its-poetry-composition-capabilities-acbb7379346f',\n",
       " 'Show HN: Qpackt – web server that can serve two versions of your website Hi guys,<p>This is very initial release of something I&#x27;ve been working on for the last few months.<p>Qpackt is an open source, Rust based, web server with several interesting features:<p>1. Basic analytics without tracking cookies.<p>2. Can serve multiple versions of your website. This can be used to A&#x2F;B tests, track users&#x27; engagement coming from different sources or gently roll users to the new version.<p>3. Auto-fetch (and renew) SSL certificates.<p>4. GUI configuration (mostly ;) ) for the ease of use.<p>It&#x27;s missing a lot of features and it&#x27;s not production ready yet. At this point I&#x27;m looking for opinions.',\n",
       " 'Show HN: Qorle – Your Personal GPT Store Hi everyone,<p>I&#x27;ve been amazed by the number of GPTs being created, and it intrigues me to contribute to the community.<p>With Qorle, you can set up your personal GPT Store, allowing you to easily share them in one place. The usage is straightforward; users only need to provide the GPT link, and the platform will automatically fetch the image, name, and description, eliminating the need for manual entry.<p>Currently, it seems the platform is unable to fetch the data. There might be something happening on OpenAI&#x27;s end that&#x27;s preventing the request, I suppose. Nevertheless, you can manually input the information. I&#x27;ll work on making it function properly and improving it over time.<p>Please let me know your thoughts, feedback, and any questions you might have. Thank you.',\n",
       " 'Ask HN: What are the noprocrast, maxvisit and minaway settings? Without fiddling with each of these to figure out what they do; what do they do?',\n",
       " 'Show HN: QR Code Scanner Working Source Code QRCScanner.com is an efficient and user-friendly online tool designed to make QR code scanning a breeze. Now, you can access and utilize its source code to integrate QR code scanning functionality into your applications, websites, or projects.',\n",
       " 'Revolutionizing Site Reliability Engineering and Digital Performance Monitoring Hello, Hacker News community,<p>I&#x27;m thrilled to introduce you to Murnitur, a pioneering startup venture to transform Site Reliability Engineering (SRE) and digital performance monitoring.<p>What is Murnitur?<p>Murnitur stands as an all-encompassing SRE platform, designed to empower businesses in monitoring, managing incidents, and optimizing on-call schedules with unparalleled efficiency. Our platform offers real-time monitoring, advanced log management, incident tracking, and data-driven insights, equipping organizations with the tools necessary to ensure their digital assets&#x27; reliability and seamless operation.<p>Why Murnitur?<p>We prioritize end-user experiences, fostering collaboration, transparency, and accountability across the digital landscape. Our commitment to innovation ensures that Murnitur remains at the forefront of technology, providing solutions that transcend the limitations of the present digital infrastructure.<p>Why You Should Try It Out:<p>Embark on a journey with us to reshape the future of digital reliability and performance. Our platform offers seamless user experiences, innovative solutions, and the promise of limitless potential in an increasingly digital world.<p>How to Get Started:<p>Visit our website https:&#x2F;&#x2F;murnitur.com to discover more about Murnitur and sign up for a free trial today. Join us as we revolutionize SRE and digital performance monitoring together!<p>We eagerly anticipate your thoughts and feedback as we continue to push the boundaries of digital reliability with Murnitur.',\n",
       " 'Show HN: AssistantHunter, a GPT that searches through 9K+ GPTs for your task There will probably be over 1M GPTs by the end of the year, so I built a search engine for GPTs.<p>It&#x27;s built with GPT custom actions and qdrant.<p>We&#x27;re accepting new GPTs, please submit yours at assistanthunt.com.<p>Enjoy building!',\n",
       " 'Show HN: Inngest – Developer platform for background jobs and workflows Hi HN! We’re Dan and Tony - founders of Inngest (<a href=\"https:&#x2F;&#x2F;www.inngest.com&#x2F;\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;www.inngest.com&#x2F;</a>). Inngest is a developer platform and toolchain for developing, testing and running background jobs, and workflows. Inngest invokes your jobs via HTTP, wherever you want to deploy your code.<p>Shipping reliable background jobs and workflows is a time suck for any software team. They’re painful to develop locally and getting into production is a tedious experience of configuring infra. When you want to add scheduling, orchestrate multi-step workflows or handle concurrency or idempotency, you spend even more time building bespoke systems - not your actual product.<p>Software engineers spend a ton of duplicated effort building and rebuilding this at every company. It shouldn’t be this way.<p>We’ve taken our experience building and scaling reliable, secure queueing systems across Healthcare, B2B SaaS, and developer infra companies. With Inngest, we sought out to create a single platform and set of developer tools to unburden the developer.<p>- You write functions alongside your API, in your existing codebase with our simple SDK. We invoke your functions via HTTPS, so there are no additional worker services to setup.<p>- End-to-end local development, with one command. Our dev server runs Inngest on any machine with a web interface to visualize, debug, and test your functions with zero additional dependencies.<p>- Our serverless queue calls you, so you can run your code anywhere - serverless, servers or edge.<p>- Inngest manages state across functions and long-running workflows for you. We handle retries, concurrency, idempotency, and coordinating parallel and sequential workloads out-of-the-box.<p>We’ve helped users like:<p>- Snaplet.dev uses Inngest to handle the lifecycle of managing preview databases for their developer platform.<p>- Ocoya.com re-build their e-commerce and social media scheduling workflows in days while dramatically simplifying their infra to run solely with Inngest + serverless functions.<p>- Secta.ai uses Inngest to run all of their AI image generation models on GPU-optimized instances.<p>Today, we have a TypeScript SDK and we will expand to other languages soon (Go is next). We’re building in the open on Github and we offer usage-based plans with a generous free tier.<p>We’re excited to share this with HN and we’re eager for your feedback! What are your experiences building systems for background jobs and workflows?',\n",
       " 'Show HN: Lucid Beads – Create Custom Bracelets in Live 3D with GPT AI Assistant Combining real-time 3D and ChatGPT to allow users to create their own custom gemstone bracelets, and get rewarded if others buy them.<p>You can use the AI Assistant for a quick start, then fine tune the design bead by bead in the Workspace if desired.<p>You don&#x27;t have to buy any of your creations, so feel free to try it out just for fun and let us know what you think!',\n",
       " 'Show HN: PyQT6 Mediator Mixin A simple mediator mixin for PyQT6.<p>Although PyQT6 has signals and slots, I found my larger projects with many widgets becoming tightly coupled. In order to help with this I implemented the mediator pattern with signals and slots.<p>I found myself repeating this in several projects, and as such decided it would be best to release a simple library that I could install in my projects, so here it is.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retrieve_filtered(\"What does Qdrant do?\", n_docs=10)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b6ce1-ad3b-42ff-835b-b9e253b3ab8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
