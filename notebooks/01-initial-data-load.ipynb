{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f912613b-cbff-4c74-900b-5a198e5bd501",
   "metadata": {},
   "source": [
    "# Filling a collection with data\n",
    "\n",
    "We'll start our experiments with a collection filled with [HackerNews](https://news.ycombinator.com/) submissions. Retrieval Augmented Generation is typically built with dense vectors, so let's try if it works in all the cases we would like to support. The [hackernews.csv](../data/hackernews.csv) is a dump of HN submissions, without the comments. Let's process it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad45c70-f24b-4b24-8d6b-0c16c5411507",
   "metadata": {},
   "source": [
    "## Setting up Qdrant collection\n",
    "\n",
    "Our collection needs to be configured for a single vector per point. Even though we have just a single vector, we'll use named vectors. If you want to use a different model, it's the time to configure it below."
   ]
  },
  {
   "cell_type": "code",
   "id": "efc6026b-976b-41b0-a473-748b0a5be0e6",
   "metadata": {},
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eadb5bd9-0b33-4634-8dc3-978418ef8acb",
   "metadata": {},
   "source": [
    "# See: https://qdrant.github.io/fastembed/examples/Supported_Models/#supported-text-embedding-models\n",
    "COLLECTION_NAME = \"hackernews-rag\"\n",
    "MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
    "VECTOR_SIZE = 384\n",
    "VECTOR_NAME = \"bge-small-en-v1.5\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b60a7349-af98-4ff2-a197-d24bca44ed28",
   "metadata": {},
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "import os\n",
    "\n",
    "client = QdrantClient(\n",
    "    os.environ.get(\"QDRANT_URL\"), \n",
    "    api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dde8961c-797b-4a5d-affe-a9fad9e1b2fe",
   "metadata": {},
   "source": [
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config={\n",
    "        VECTOR_NAME: models.VectorParams(\n",
    "            size=VECTOR_SIZE,\n",
    "            distance=models.Distance.COSINE,\n",
    "        )\n",
    "    },\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "48f3808e-6f4b-442f-a6e6-ae63ae9fc4b6",
   "metadata": {},
   "source": [
    "## Processing input data\n",
    "\n",
    "Our dataset is a regular CSV file we need to iterate over and store in Qdrant. We'll use a local inference mode based on Qdrant<>FastEmbed integration, so we don't need to compute the vectors separately, but can just pass raw data and expect the client to convert it."
   ]
  },
  {
   "cell_type": "code",
   "id": "5525a6c7-0da9-4044-9623-0b38cb8819fd",
   "metadata": {},
   "source": [
    "import csv\n",
    "\n",
    "with open(\"../data/hackernews.csv\", newline=\"\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    row = next(reader)\n",
    "    print(row)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d58bf1cc-15b2-4087-8ccf-29333a296c39",
   "metadata": {},
   "source": [
    "from itertools import batched\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "with open(\"../data/hackernews.csv\", newline=\"\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for batch in tqdm(batched(reader, n=16)):\n",
    "        client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=[\n",
    "                models.PointStruct(\n",
    "                    # HackerNews id is Qdrant id as well\n",
    "                    id=int(point[\"id\"]),\n",
    "                    vector={\n",
    "                        VECTOR_NAME: models.Document(\n",
    "                            text=f\"{point['title']} {point['text']}\",\n",
    "                            model=MODEL_NAME,\n",
    "                        )\n",
    "                    },\n",
    "                    payload={\n",
    "                        \"datetime\": datetime.utcfromtimestamp(int(point[\"time\"])).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "                        **point\n",
    "                    },\n",
    "                )\n",
    "                for point in batch\n",
    "            ]\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6e98b207-a6a5-4452-9856-16e0ce51d612",
   "metadata": {},
   "source": [
    "client.recover_snapshot(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    # Please do not modify the URL below\n",
    "    location=\"https://storage.googleapis.com/tutorials-snapshots-bucket/workshop-improving-r-in-rag/hackernews-rag.snapshot\",\n",
    "    wait=False, # Loading a snapshot may take some time, so let's avoid a timeout\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c6cc7663-14bc-4268-ae68-fc26cf05b6b3",
   "metadata": {},
   "source": [
    "## Building RAG with Qdrant-based retrieval\n",
    "\n",
    "Let's build a naive RAG with dense vector search. It'll be a very basic process, using the original prompt as a query and then passes retrieved context to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "id": "9f38b3d9-b6cd-4875-a94e-26526e16f1ff",
   "metadata": {},
   "source": [
    "from any_llm import list_models\n",
    "\n",
    "list_models(provider=os.environ.get(\"LLM_PROVIDER\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f890a1a7-bb7e-4409-b38b-6a52d7685298",
   "metadata": {},
   "source": [
    "LLM_NAME = \"claude-sonnet-4-20250514\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b19a9a4b-34ff-4bc5-ac75-06b10863dc49",
   "metadata": {},
   "source": [
    "def retrieve(q: str, n_docs: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve documents based on the provided query\n",
    "    \"\"\"\n",
    "    result = client.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query=models.Document(\n",
    "            text=q,\n",
    "            model=MODEL_NAME,\n",
    "        ),\n",
    "        using=VECTOR_NAME,\n",
    "        limit=n_docs,\n",
    "    )\n",
    "    docs = [\n",
    "        f\"{point.payload['title']} {point.payload['text']}\"\n",
    "        for point in result.points\n",
    "    ]\n",
    "    return docs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "47ebe9f7-59a5-412f-92bc-63ba94359df7",
   "metadata": {},
   "source": [
    "retrieve(\"What are the coolest ideas for an AI startup?\", n_docs=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "03c5a0d8-20b6-402b-8de8-824172056ecd",
   "metadata": {},
   "source": [
    "### Payload-based filtering\n",
    "\n",
    "HackeNews submissions may have just a title, but in such a case they rarely provide any useful information. It seems to make sense to exclude such submissions entirely, and only focus on the ones having some more details than just the submission title."
   ]
  },
  {
   "cell_type": "code",
   "id": "77e4aece-4cec-4e17-ae64-092043ac6f8d",
   "metadata": {},
   "source": [
    "client.create_payload_index(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    field_name=\"text\",\n",
    "    field_schema=\"keyword\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "76c2a110-ef99-42b5-b2d6-8573352ae175",
   "metadata": {},
   "source": [
    "def retrieve_filtered(q: str, n_docs: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve documents based on the provided query,\n",
    "    but only those which have non-empty text attribute.\n",
    "    \"\"\"\n",
    "    result = client.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query=models.Document(\n",
    "            text=q,\n",
    "            model=MODEL_NAME,\n",
    "        ),\n",
    "        query_filter=models.Filter(\n",
    "            must_not=[\n",
    "                # Lack of field\n",
    "                models.IsEmptyCondition(\n",
    "                    is_empty=models.PayloadField(key=\"text\"),\n",
    "                ),\n",
    "                # Field set to null value\n",
    "                models.IsNullCondition(\n",
    "                    is_null=models.PayloadField(key=\"text\"),\n",
    "                ),\n",
    "                # Field set to an empty string\n",
    "                models.FieldCondition(\n",
    "                    key=\"text\",\n",
    "                    match=models.MatchValue(value=\"\"),\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "        using=VECTOR_NAME,\n",
    "        limit=n_docs,\n",
    "    )\n",
    "    docs = [\n",
    "        f\"{point.payload['title']} {point.payload['text']}\"\n",
    "        for point in result.points\n",
    "    ]\n",
    "    return docs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fe4be1f8-ee66-4405-9a6b-c36519ab4db2",
   "metadata": {},
   "source": [
    "retrieve_filtered(\"What are the coolest ideas for an AI startup?\", n_docs=10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9eacef8-5970-4364-b124-bcac718917a9",
   "metadata": {},
   "source": [
    "from any_llm import acompletion\n",
    "from typing import Callable\n",
    "\n",
    "RetieverFunc = Callable[[str, int], list[str]]\n",
    "\n",
    "\n",
    "async def rag(q: str, retrieve_func: RetieverFunc, *, n_docs: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Run single-turn RAG on a given input query.\n",
    "    Return just the model response.\n",
    "    \"\"\"\n",
    "    docs = retrieve_func(q, n_docs)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Please provide a response to my question based only \" +\n",
    "                \"on the provided context and only it. If it doesn't \" +\n",
    "                \"contain any helpful information, please let me know \" +\n",
    "                \"and admit you cannot produce relevant answer.\\n\" +\n",
    "                f\"<context>{'\\n'.join(docs)}</context>\\n\" +\n",
    "                f\"<question>{q}</question>\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    response = await acompletion(\n",
    "        provider=os.environ.get(\"LLM_PROVIDER\"),\n",
    "        model=LLM_NAME,\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0eaf9be1-9391-4127-8257-219df24079b1",
   "metadata": {},
   "source": [
    "response = await rag(\n",
    "    \"What are the coolest ideas for an AI startup?\", \n",
    "    retrieve_func=retrieve_filtered\n",
    ")\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28bba782-eb93-415c-9c79-a62ec67f479b",
   "metadata": {},
   "source": [
    "response = await rag(\"What does Qdrant do?\", retrieve_func=retrieve_filtered)\n",
    "print(response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d6adc233-d8da-4b30-883c-22c28c6ee3dd",
   "metadata": {},
   "source": [
    "docs = retrieve_filtered(\"What does Qdrant do?\", n_docs=10)\n",
    "docs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5a6b6ce1-ad3b-42ff-835b-b9e253b3ab8a",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
