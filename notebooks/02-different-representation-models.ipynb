{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa31f560-d5ab-4bbd-84e0-605c635f0deb",
   "metadata": {},
   "source": [
    "# Testing different representation methods\n",
    "\n",
    "Dense retrieval is easy to start with, but it does not provide the most accurate answers in all the cases. Sometimes, we need exact keyword matching and in that cases sparse vectors, such as BM25 might be more appropriate. They definitely excel at proper names detection, and we may need that to search over our datasets, with specific company constraints in mind. Let's add another representation method and build hybrid search using both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277e8b4c-7178-48c9-b547-b72cc058d059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e4c55f-ce6a-4e22-b499-424293579e7a",
   "metadata": {},
   "source": [
    "## Setting up another Qdrant collection for multiple vectors per point\n",
    "\n",
    "If we want to use different search methods, we need to store multiple vectors per point in one collection. It's easier that way, as multi-stage retrieval pipelines might be launched in a single API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae288b6c-10de-40fd-81c9-f421bbf0ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://qdrant.github.io/fastembed/examples/Supported_Models/#supported-text-embedding-models\n",
    "COLLECTION_NAME = \"hackernews-hybrid-rag\"\n",
    "\n",
    "# Dense retrieval\n",
    "MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
    "VECTOR_SIZE = 384\n",
    "VECTOR_NAME = \"bge-small-en-v1.5\"\n",
    "\n",
    "# Sparse model\n",
    "BM25_MODEL_NAME = \"Qdrant/bm25\"\n",
    "BM25_VECTOR_NAME = \"bm25\"\n",
    "\n",
    "# Token-level representations\n",
    "MUTLIVECTOR_MODEL_NAME = \"colbert-ir/colbertv2.0\"\n",
    "MULTIVECTOR_SIZE = 128\n",
    "MULTIVECTOR_NAME = \"colbertv2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36076f5e-37f0-4433-a553-31ffbf550b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "import os\n",
    "\n",
    "client = QdrantClient(\n",
    "    os.environ.get(\"QDRANT_URL\"), \n",
    "    api_key=os.environ.get(\"QDRANT_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd39448a-c71b-40d5-9bde-e8d69c470328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config={\n",
    "        VECTOR_NAME: models.VectorParams(\n",
    "            size=VECTOR_SIZE,\n",
    "            distance=models.Distance.COSINE,\n",
    "        ),\n",
    "        MULTIVECTOR_NAME: models.VectorParams(\n",
    "            size=MULTIVECTOR_SIZE,j\n",
    "            distance=models.Distance.DOT,\n",
    "            multivector_config=models.MultiVectorConfig(\n",
    "                comparator=models.MultiVectorComparator.MAX_SIM\n",
    "            ),\n",
    "            # Disable HNSW for reranking\n",
    "            hnsw_config=models.HnswConfigDiff(m=0),\n",
    "        ),\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        BM25_VECTOR_NAME: models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF,\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5473c79-a010-4ad5-94b0-ced242ec5217",
   "metadata": {},
   "source": [
    "## Migrating to multiple vectors\n",
    "\n",
    "There is no need to recreate the previously created dense embeddings, as we can migrate them from the previous collection and avoid recomputations. In the meantime we'll still create sparse and multi-vector representations. Also, since we agreed we need more context, we don't really need to store the points without more detailed text description of a submission, so let's filter them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69ca4905-40b0-46be-8b7e-6c70efce358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_COLLECTION_NAME = \"hackernews-rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "212cede3-1029-48f7-a33b-e7516e779ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6020d8d737f4878a34e77cb2500f19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afd1865dfc248dc97457e4b7b571504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "danish.txt:   0%|          | 0.00/424 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac4899421424675b508dc991593e5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a55b0cba7574a13be7ae0b1f1fa1daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "english.txt:   0%|          | 0.00/936 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f1f382a4dc4da78bbad8becd129a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "french.txt:   0%|          | 0.00/813 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b69f6a3e1f41e2ba191f080188abfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dutch.txt:   0%|          | 0.00/453 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c17fd7830964ac59d621dd233fd2356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "arabic.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95645ce27a74747bd91b332309ec84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "german.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093e64fe7a6e4883a1ab82ac4ab3e8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "finnish.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e203552dce4de28be56013fa9de894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hungarian.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5077abd0d5f44e44b5d087f17f7178e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "greek.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a88f40403ab493d9d4ce22fc03e85ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "italian.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628f952114344d8084a94e69f07341f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "norwegian.txt:   0%|          | 0.00/851 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1726252ca394788bee1d657770dc35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "russian.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f708413238df4af2a1922d694e5d2038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "romanian.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b22c04eaca4fe2a8583455f44b583e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spanish.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2beb2530d544ec8d520534e59cf625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "swedish.txt:   0%|          | 0.00/559 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c132c2b624490ba849ec6614312930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "turkish.txt:   0%|          | 0.00/260 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022795edfc6e4b038fadffad2f5b6da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "portuguese.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e131df63cd1c4725961e04953a09560c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d46a53a06f4a989df57d22ce1853c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfbfc0277bdd470d9386f3ac0cd25e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fc053f8cfc40d3b7a1b5f0d59e2055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29b126b15ea4364bb88946271577ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da87c104b7f14284bd9ab20a23b11b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "last_offset = None\n",
    "while True:\n",
    "    # Get a batch of records\n",
    "    records, last_offset = client.scroll(\n",
    "        collection_name=OLD_COLLECTION_NAME, \n",
    "        scroll_filter=models.Filter(\n",
    "            must_not=[\n",
    "                # Lack of field\n",
    "                models.IsEmptyCondition(\n",
    "                    is_empty=models.PayloadField(key=\"text\"),\n",
    "                ),\n",
    "                # Field set to null value\n",
    "                models.IsNullCondition(\n",
    "                    is_null=models.PayloadField(key=\"text\"),\n",
    "                ),\n",
    "                # Field set to an empty string\n",
    "                models.FieldCondition(\n",
    "                    key=\"text\",\n",
    "                    match=models.MatchValue(value=\"\"),\n",
    "                ),\n",
    "            ],\n",
    "        ),\n",
    "        offset=last_offset,\n",
    "        with_payload=True,\n",
    "        with_vectors=True,\n",
    "        limit=10,\n",
    "    )\n",
    "\n",
    "    # Migrate them to a new collection\n",
    "    client.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[\n",
    "            models.PointStruct(\n",
    "                id=record.id,\n",
    "                vector={\n",
    "                    # Copy the dense embedding directly\n",
    "                    VECTOR_NAME: record.vector[VECTOR_NAME],\n",
    "                    # Calculate BM25 embedding\n",
    "                    BM25_VECTOR_NAME: models.Document(\n",
    "                        text=f\"{record.payload['title']} {record.payload['text']}\",\n",
    "                        model=BM25_MODEL_NAME,\n",
    "                    ),\n",
    "                    # Calculate ColBERT embeddings as well\n",
    "                    MULTIVECTOR_NAME: models.Document(\n",
    "                        text=f\"{record.payload['title']} {record.payload['text']}\",\n",
    "                        model=MUTLIVECTOR_MODEL_NAME,\n",
    "                    ),\n",
    "                },\n",
    "                payload=record.payload,\n",
    "            )\n",
    "            for record in records\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Stop when the last batch has been already processed\n",
    "    if last_offset is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd9a15-e784-4c37-a65a-f24cdb671613",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.recover_snapshot(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    # Please do not modify the URL below\n",
    "    location=\"https://storage.googleapis.com/tutorials-snapshots-bucket/workshop-improving-r-in-rag/hackernews-hybrid-rag.snapshot\",\n",
    "    wait=False, # Loading a snapshot may take some time, so let's avoid a timeout\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9cdc01-da71-40ac-af3a-ea1ba72b601a",
   "metadata": {},
   "source": [
    "## Experimenting with Hybrid Search\n",
    "\n",
    "Our previous attempts to use dense retrieval to find some Qdrant-specific data weren't succesful. Let's try to build a better retriever that will use keyword-based search retrieval and dense reranking, so it hopefully capture more nuances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69a57a86-eafd-4038-a013-d5c11ea83301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_dual(q: str, n_docs: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve documents based on the provided query\n",
    "    with BM25 retrieval and dense reranking.\n",
    "    \"\"\"\n",
    "    result = client.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=q,\n",
    "                    model=BM25_MODEL_NAME,\n",
    "                ),\n",
    "                using=BM25_VECTOR_NAME,\n",
    "                # Prefetch ten times more!\n",
    "                limit=(n_docs * 10)\n",
    "            ),\n",
    "        ],\n",
    "        query=models.Document(\n",
    "            text=q,\n",
    "            model=MODEL_NAME,\n",
    "        ),\n",
    "        using=VECTOR_NAME,\n",
    "        limit=n_docs,\n",
    "    )\n",
    "    docs = [\n",
    "        f\"{point.payload['title']} {point.payload['text']}\"\n",
    "        for point in result.points\n",
    "    ]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43ef142c-4ddb-4a1a-baf6-6fdb649f5dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How to Augment GPT-4 with Qdrant to Elevate Its Poetry Composition Capabilities GPT-4 and Qdrant synergize, transforming poetry with enhanced coherence and depth. Visit my medium article to view the code implementation: https:&#x2F;&#x2F;medium.com&#x2F;@akriti.upadhyay&#x2F;how-to-augment-gpt-4-with-qdrant-to-elevate-its-poetry-composition-capabilities-acbb7379346f',\n",
       " 'Show HN: AssistantHunter, a GPT that searches through 9K+ GPTs for your task There will probably be over 1M GPTs by the end of the year, so I built a search engine for GPTs.<p>It&#x27;s built with GPT custom actions and qdrant.<p>We&#x27;re accepting new GPTs, please submit yours at assistanthunt.com.<p>Enjoy building!',\n",
       " 'Show HN: VectorAdmin – An open-source vector database management system Hey HN,<p>At Mintplex Labs are building developer tools for AI applications. One area we encountered frustration was the use of Vector Databases like Pinecone, Chroma, QDrant, or Weaviate to &quot;unlock&quot; long-term memory and contextual answers. It is nearly impossible to manage this data when in use for production.<p>The craziest thing was how you cannot atomically CRUD any vectors in most of these vector databases. Let alone easily copy, clone, or migrate data or entire indexes without paying for re-embedding - among other things.<p>With VectorAdmin you get a database level UI with the ability to easily search for embeddings and atomically manage them on top of being a general tool suite for those using vector databases with LLMs.<p>Some things we have unlocked with Vector Admin:\\n- Upload data directly into the vector db via a text doc or PDF\\n- One click sync of entire existing vector databases\\n- Migrating entire db&#x27;s to another provider to escape vendor lock in\\n- Ability to duplicate collections&#x2F;namespaces to create dev-environments off production data at no cost\\n- Be able to finally reset a vector database (provider agnostic)<p>and soon, be able to detect &quot;drift&quot; in semantic search results and catch it early before your production system starts providing wild context snippets.<p>VectorAdmin is open-source or hosted and has a 3-day trial. We really want HN&#x27;s feedback on the issues or problem you are having wrangling the actual data in a vector database while building any LLM application.',\n",
       " 'Show HN: Simgen – a minimal content recommendation engine for your static site # What is simgen-ssg?<p>`simgen-ssg` is a content recommendation engine tailored specifically for static sites. Built on top of Qdrant and fastembed, it enables seamless integration of personalized ‘you might also like’ sections for your blog, FAQs, Portfolios, and various content-driven pages.<p>Have a look at the blog explaining it a little further and how I plug it in my workflow. <a href=\"https:&#x2F;&#x2F;sharmashobh.it&#x2F;blog&#x2F;simgen-ssg\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;sharmashobh.it&#x2F;blog&#x2F;simgen-ssg</a><p>Also check out the blog that inspired this idea: <a href=\"https:&#x2F;&#x2F;til.simonwillison.net&#x2F;llms&#x2F;openai-embeddings-related-content\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;til.simonwillison.net&#x2F;llms&#x2F;openai-embeddings-related...</a>',\n",
       " 'Ask HN: Kubernetes ingress re-write for FEs What is the proper way to set rewrite rules that can handles requests after first load, For example &lt;HTML&gt; relative path requests for assets or hrefs?<p>Lets say you have homebase.com&#x2F;kubedash and homebase.com&#x2F;qdrant, what is the best practice for supporting these two services.<p>- homebase.com&#x2F;qdrant --&gt;  qdrant:port&#x2F;<p>- homebase.com&#x2F;qdrant&#x2F;dashboard --&gt;  qdrant:port&#x2F;dashboard<p>- homebase.com&#x2F;kubedash --&gt; kubernetes-dashboard:80&#x2F;<p>The problem i run into is that each of the services load an HTML with relative paths to the &#x27;re-written&#x27; request. In other words qdrant:port&#x2F;dashboard  HTML will try to fetch `homebase.com&#x2F;dashboard&#x2F;assets` but your re-write rule would only handle it if its, `homebase.com&#x2F;qdrant&#x2F;dashboard&#x2F;assets`<p>There are many answers and suggestions but i&#x27;ve worked through most and have realized that either your href or your assets or something else will break it. The solution I have found is to use subdomains.<p>What is the right way to do this? This feels like the basic feature for ingress controller.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_dual(\"What does Qdrant do?\", n_docs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "191de14d-70b3-43b6-8efa-07d777e2ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from any_llm import acompletion\n",
    "from typing import Callable\n",
    "\n",
    "RetieverFunc = Callable[[str, int], list[str]]\n",
    "\n",
    "\n",
    "async def rag(q: str, retrieve_func: RetieverFunc, *, n_docs: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Run single-turn RAG on a given input query.\n",
    "    Return just the model response.\n",
    "    \"\"\"\n",
    "    docs = retrieve_func(q, n_docs)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Please provide a response to my question based only \" +\n",
    "                \"on the provided context and only it. If it doesn't \" +\n",
    "                \"contain any helpful information, please let me know \" +\n",
    "                \"and admit you cannot produce relevant answer.\\n\" +\n",
    "                f\"<context>{'\\n'.join(docs)}</context>\\n\" +\n",
    "                f\"<question>{q}</question>\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "    response = await acompletion(\n",
    "        provider=os.environ.get(\"LLM_PROVIDER\"),\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "455d4ec8-dcc8-4c2d-a8e0-cb95b6ce597b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/15/25 17:18:49] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> max_tokens is required for Anthropic, setting to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span>                     <a href=\"file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py#279\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">279</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/15/25 17:18:49]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m max_tokens is required for Anthropic, setting to \u001b[1;36m8192\u001b[0m                     \u001b]8;id=667078;file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py\u001b\\\u001b[2mutils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=24349;file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py#279\u001b\\\u001b[2m279\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, Qdrant is a vector database that has several capabilities:\n",
      "\n",
      "1. **Vector Database Functionality**: Qdrant is mentioned alongside other vector databases like Pinecone, Chroma, and Weaviate as a tool used to \"unlock\" long-term memory and contextual answers in AI applications.\n",
      "\n",
      "2. **Integration with AI Models**: It can be integrated with GPT-4 to enhance poetry composition capabilities by providing \"enhanced coherence and depth.\"\n",
      "\n",
      "3. **Search Engine Backend**: Qdrant is used as part of the infrastructure for building search engines, such as the GPT search engine mentioned that searches through thousands of GPTs.\n",
      "\n",
      "4. **Content Recommendation**: It serves as the foundation for content recommendation engines, specifically mentioned as being used with fastembed to create personalized \"you might also like\" sections for static sites.\n",
      "\n",
      "5. **Embedding Management**: The context suggests it stores and manages embeddings (vector representations of data) that can be searched and manipulated for various AI and machine learning applications.\n",
      "\n",
      "From the context provided, Qdrant appears to be a vector database solution primarily used in AI applications for semantic search, content recommendations, and enhancing language model capabilities through vector-based data storage and retrieval.\n"
     ]
    }
   ],
   "source": [
    "response = await rag(\n",
    "    \"What does Qdrant do?\", \n",
    "    retrieve_func=retrieve_dual\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cb7968e-e57e-4827-b79c-80ead2811117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Show HN: Quickwit – Cost-Efficient OSS Search Engine for Observability Hi HN, I’m one of the builders of Quickwit, a cloud-native OSS search engine for observability. As of 2023, we support logs and traces, metrics will come in 2024.<p>You know the pitch: while software like Datadog or Splunk are great, they often comes with hefty price tags. Our mission is to offer an affordable alternative. So we’ve built Quickwit, we’ve made it compatible with the observabilty ecosystem (OpenTelemetry, Jaeger, Grafana) and above all, we’ve made it cost-efficient &#x2F; “easy” to scale (well it’s never easy to scale to petabytes..).<p>To give you a glance at the engine performance I made a benchmark on the GitHub Archive dataset, 23 TB of events, here are the main observations:<p>Indexing: costs $2 per ingested TB. With 4CPU, throughput is at 20MBs However, you&#x27;ll observe &gt; 30MB throughput on simpler datasets, like logs and traces.<p>Search: a typical query costs $0.0002 per TB (considering both CPU time and GET request costs). Using 8CPU, a simple query on 23TB is achieved in under a second.<p>Storage: on S3, it costs $8 per ingested TB per month on the GitHub Archive dataset. With logs and traces, you might see costs around $5&#x2F;ingested TB due to a 2x better compression ratio.<p>I&#x27;m eager to get your thoughts on this!<p>Benchmark: <a href=\"https:&#x2F;&#x2F;quickwit.io&#x2F;blog&#x2F;benchmarking-quickwit-engine-on-an-adversarial-dataset\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;quickwit.io&#x2F;blog&#x2F;benchmarking-quickwit-engine-on-an-...</a><p>Github repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;quickwit-oss&#x2F;quickwit&#x2F;\">https:&#x2F;&#x2F;github.com&#x2F;quickwit-oss&#x2F;quickwit&#x2F;</a><p>Website: <a href=\"https:&#x2F;&#x2F;quickwit.io&#x2F;\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;quickwit.io&#x2F;</a>',\n",
       " 'Show HN: An open source extension to block large media brands from Google search Google sends 16 large media brands (588 individual brands) a combined 3 billion+ clicks per month. They rank first page on 85% of all searches.<p>They are inundating the web with things like subpar product recommendations, AI written listicles, and cookie cutter reviews.<p>I don&#x27;t like that, so I made an open source extension that blocks all of them from search results. (I&#x27;m not a developer, and I made this with Claude Opus&#x27;s help)',\n",
       " 'Show HN: Search on S3 Using AWS Lambda Hi everyone,<p>During the last 3 months, I worked on the Quickwit search engine to adapt it for AWS Lambda runtime. I wrote a blog post to announce the beta release [0] and two tutorials to quickstart:<p>- Indexing and searching 20 million logs entries dataset [1].<p>- Implementing an E2E use case where an application generates data, uploads it in batches to S3. Users can then search through an HTTP API authenticated with an API Key [2].<p>On the performance side, I observed on a 20 million log dataset that search is sub-second, and analytics queries take from 1 to 4 second.<p>On the cost side, obviously the compute using Lambda is fairly expensive, but the huge win is that it scales to 0. Few other solutions do. For instance OpenSearch Serverless has a fixed cost around $700&#x2F;months. On the search side, thanks to indexing, queries are usually way cheaper than scanning services like CloudWatch (at least 50x cheaper).<p>I would love to get the feedback from the community and see how I can push this further.<p>[0]: Blog post <a href=\"https:&#x2F;&#x2F;quickwit.io&#x2F;blog&#x2F;quickwit-lambda-beta\" rel=\"nofollow\">https:&#x2F;&#x2F;quickwit.io&#x2F;blog&#x2F;quickwit-lambda-beta</a><p>[1]: 20 million log dataset search tutorial <a href=\"https:&#x2F;&#x2F;quickwit.io&#x2F;docs&#x2F;get-started&#x2F;tutorials&#x2F;tutorial-aws-lambda-simple\" rel=\"nofollow\">https:&#x2F;&#x2F;quickwit.io&#x2F;docs&#x2F;get-started&#x2F;tutorials&#x2F;tutorial-aws-...</a><p>[2]: E2E use case tutorial <a href=\"https:&#x2F;&#x2F;quickwit.io&#x2F;docs&#x2F;guides&#x2F;e2e-serverless-aws-lambda\" rel=\"nofollow\">https:&#x2F;&#x2F;quickwit.io&#x2F;docs&#x2F;guides&#x2F;e2e-serverless-aws-lambda</a>',\n",
       " 'Show HN: Neum AI – Open-source large-scale RAG framework Over the last couple months we have been supporting developers in building large-scale RAG pipelines to process millions of pieces of data.<p>We documented our approach in an HN post (<a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37824547\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37824547</a>) a couple weeks ago. Today, we are open sourcing the framework we have developed.<p>The framework focuses on RAG data pipelines and provides scale, reliability, and data synchronization capabilities out of the box.<p>For those newer to RAG, it is a technique to provide context to Large Language Models. It consists of grabbing pieces of information (i.e. pieces of news articles, papers, descriptions, etc.) and incorporating them into prompts to help contextualize the responses. The technique goes one level deeper in finding the right pieces of information to incorporate. The search for relevant information is done through the use of vector embeddings and vector databases.<p>Those pieces of news articles, papers, etc. are transformed into a vector embedding that represents the semantic meaning of the information. These vector representations are organized into indexes where we can quickly search for the pieces of information that most closely resembles (from a semantic perspective) a given question or query. For example, if I take news articles from this year, vectorize them, and add them to an index, I can quickly search for pieces of information about the US elections.<p>To help achieve this, the Neum AI framework features:<p>Starting with built-in data connectors for common data sources, embedding services and vector stores, the framework provides modularity to build data pipelines to your specification.<p>The connectors support pre-processing capabilities to define loading, chunking and selecting strategies to optimize content to be embedded. This also includes extracting metadata that is going to be associated to a given vector.<p>The generated pipelines support large scale jobs through a high throughput distributed architecture. The connectors allow you to parallelize tasks like downloading documents, processing them, generating embedding and ingesting data into the vector DB.<p>For data sources that might be continuously changing, the framework supports data scheduling and synchronization. This includes delta syncs where only new data is pulled.<p>Once data is transformed into a vector database, the framework supports querying of the data including hybrid search using the available metadata added during pre-processing. As part of the querying process, the framework provides capabilities to capture feedback on retrieved data as well as run evaluations against different pipeline configurations.<p>Try it out and if interested in chatting more about this shoot us an email founders@tryneum.com',\n",
       " 'Ask HN: Does there exist a read-optimized distributed K-V store? I have a large-ish (right now ~billions of keys, ~100TB) immutable&#x2F;append-only dataset, with rare inserts (~1GB chunks, once every few minutes) done only by background bulk insert, and no need for any kind of consistency (i.e. inserted data isn&#x27;t &quot;from&quot; users, and so nobody is expecting inserted data to be reflected immediately; and keys inserted together don&#x27;t even need to become visible atomically, as clients will retry to fetch dependent keys.)<p>And I need to serve huge numbers (~trillions) of single-key point queries out of this dataset, out to many consumers over the Internet, with each fetch having as little per-fetch read latency as possible — ideally, with round-trip times resembling e.g. a Redis GET.<p>Basically, I want the performance properties that you&#x27;d get in a single-node scenario from using LMDB and dedicating all your RAM to OS disk cache — but &quot;in the large&quot;, with data being sharded into vnodes and then those vnodes being spread+replicated across an elastically auto-scaled set of compute replicas.<p>You&#x27;d think the answer would be a thing that calls itself something like a &quot;distributed KV store&quot;, &quot;serverless NoSQL store&quot;, etc.<p>But it seems that all the big &quot;distributed KV store&quot; products and services — DynamoDB, BigTable, Cassandra, Riak KV, FoundationDB, CockroachDB, ScyllaDB, etc. — are all built to optimize for <i>write</i> throughput in a many-distributed-writers use-case, with little concern for per-read latency. They&#x27;re all &quot;LevelDB in the large&quot;, not &quot;LMDB in the large.&quot;<p>Does HN know of any system that <i>would</i> suit my use-case? Or, if not, any ideas why nobody has built one?',\n",
       " 'Show HN: ChatGPT for Your Data I built an ai agent that can connect to your database.\\nThere&#x27;s a lot of text-to-sql ai tools out there, here are the 2 key differences about my approach:\\n1. AI Agent - I give the Agent a bunch of tools and let it decide what tools to call, in what order, with which parameters, in order to help the user. this means you can ask super vague questions and the agent helps you think through coming up with a good answer.\\n2. Metadata graph &amp; embeddings - I use an LLM to create metadata about the db schemas including things like join keys, column descriptions, and table contents. I also convert this metadata into embeddings. this powers a kNN search that the agent can use to find the right data in the db.<p>if you would be willing to give it a try please put in your email on the page. thanks!',\n",
       " 'Show HN: Large Scale AI Blueprint Comprehensive Large Scale AI Systems Blueprint Guide and Roadmap on GitHub.',\n",
       " 'Ask HN: How to optimise LLM embeddings for search I&#x27;ve seen some people discuss on here about some techniques they have used to get more accuracy from using embeddings to perform search. For example, I have seen some suggest that extracting keywords from the text and creating an embedding out of those can work better. I was wondering if anyone had any articles testing these methods out; or if there were other methods people know about.',\n",
       " 'Ask HN: How would you structure a CRM for scale? I&#x27;m curious to hear how you would structure the architecture for an email marketing&#x2F;CRM service for scale. Let&#x27;s say you have 5,000 customers. Some customers have 5,000 or less contacts, some have 100,000+, some have millions.<p>Are you storing all their data on one large database? Multiple databases? If multiple databases is there a specific way you&#x27;re splitting up the data across these databases?<p>If multiple databases how are you ensuring search in your app is extremely fast to return any datapoint a customer is looking for on a customer record?<p>When it comes to email sending, how are you handling it? Lets say you have 100 customers at once send an email blast that consists of 30MM emails. What queue service are you using? What about analytics to track all of the data? Are you storing that data on separate boxes?<p>What about replication? Set it up on all boxes?<p>Curious to hear how people would handle a setup like this that will remain extremely fast, all data is quickly searchable from the UI, application is always stable and does not slow down under heavy write operations, and if there is downtime maybe the entire app is not offline but only a few databases that contain only some customers data.',\n",
       " 'Show HN: Biblos – Semantic Search the Church Fathers I&#x27;m pleased to present an update to Biblos, a semantic search tool designed for biblical research. This release incorporates instructor-large embeddings to enhance the precision of verse retrieval. Introducing some key features including historical church writings and commentaries alongside the main biblical corpus. Available for use at <a href=\"https:&#x2F;&#x2F;biblos.app&#x2F;\" rel=\"nofollow noreferrer\">https:&#x2F;&#x2F;biblos.app&#x2F;</a><p>Technical Insights:<p>- Utilizes Chroma for vector search, now powered by instructor-large embeddings for improved semantic accuracy.<p>- Features Anthropic&#x27;s Claude LLM model to generate summaries that provide context and clarity for search results.<p>- Developed with a Retrieval Augmented Generation (RAG) architecture, the app offers a streamlined user experience through a Streamlit Web UI, all orchestrated with Python.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_dual(\"How do I perform a KNN search on a large scale?\", n_docs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af463d-66c8-495a-b472-f75629457c61",
   "metadata": {},
   "source": [
    "### More sophisticated reranking\n",
    "\n",
    "Sparse retrieval and dense reranking might be a useful strategy, but it cannot support all the possible search queries. If we cannot capture a particular semantic match using sparse vectors, then dense reranking won't even see it, so it'll never get retrieved. That's why it pretty common to use both methods for prefetching, and something else for reranking, so we can have the best of both worlds.\n",
    "\n",
    "In the simplest case, we can run both prefetches and combine the results with fusion based on the ranks as returned by the individual methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dabcf0f-abb2-4edb-86b1-b5c7ff61cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_fusion(q: str, n_docs: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve documents based on the provided query\n",
    "    with BM25 and dense retrieval + fusion to merge them.\n",
    "    \"\"\"\n",
    "    result = client.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=q,\n",
    "                    model=BM25_MODEL_NAME,\n",
    "                ),\n",
    "                using=BM25_VECTOR_NAME,\n",
    "                limit=n_docs,\n",
    "            ),\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=q,\n",
    "                    model=MODEL_NAME,\n",
    "                ),\n",
    "                using=VECTOR_NAME,\n",
    "                limit=n_docs,\n",
    "            ),\n",
    "        ],\n",
    "        # Reciprocal Rank Fusion works on the rankings\n",
    "        query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        limit=n_docs,\n",
    "    )\n",
    "    docs = [\n",
    "        f\"{point.payload['title']} {point.payload['text']}\"\n",
    "        for point in result.points\n",
    "    ]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93f0663a-65e8-4380-9467-bfdfe774c1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Show HN: GritQL, a Rust CLI for rewriting source code Hi everyone!<p>I’m excited to open source GritQL, a Rust CLI for searching and transforming source code.<p>GritQL comes from my experiences with conducting large scale refactors and migrations.<p>Usually, I would start exploring a codebase with grep. This is easy to start with, but most migrations end up accumulating additional requirements like ensuring the right packages are imported and excluding cases which don’t have a viable migration path.<p>Eventually, to build a complex migration, I usually ended up having to write a full codemod program with a tool like jscodeshift. This comes with its own problems:<p>- Most of the exploratory work has to be abandoned as you figure out how to represent your original regex search as an AST.\\n- Reading&#x2F;writing a codemod requires mentally translating from AST names back to what source code actually looks like.\\n- Performance is often an afterthought, so iterating on a large codemod can be painfully slow.\\n- Codemod frameworks are language-specific, so if you’re hopping between multiple languages—or trying to migrate a shared API—you have to learn different tools.<p>GritQL is an attempt to develop a powerful middle ground:\\n- Exploratory analysis is easy: just put a code snippet in backticks and use $metavariables for placeholders.\\n- Incrementally add complexity by introducing side conditions with where clauses.\\n- Reuse named patterns to avoid rebuilding queries, and use shared patterns from our standard library for common tasks like ensuring modules are imported.\\n- Iterate on large codebases quickly: we use Rust for maximum performance<p>GritQL has already been used on thousands of repositories for complex migrations[1] but we&#x27;re excited to collaborate more with the open source community.<p>[1] Ex. <a href=\"https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;openai-python&#x2F;discussions&#x2F;742\">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;openai-python&#x2F;discussions&#x2F;742</a>',\n",
       " 'Ask HN: What is the state of art approximate k-NN search algorithm today? For someone interested in learning more about search, what kind of ANN does big search company like google use?<p>IVF, LSH or even HNSW?',\n",
       " 'Show HN: ChatGPT for Your Data I built an ai agent that can connect to your database.\\nThere&#x27;s a lot of text-to-sql ai tools out there, here are the 2 key differences about my approach:\\n1. AI Agent - I give the Agent a bunch of tools and let it decide what tools to call, in what order, with which parameters, in order to help the user. this means you can ask super vague questions and the agent helps you think through coming up with a good answer.\\n2. Metadata graph &amp; embeddings - I use an LLM to create metadata about the db schemas including things like join keys, column descriptions, and table contents. I also convert this metadata into embeddings. this powers a kNN search that the agent can use to find the right data in the db.<p>if you would be willing to give it a try please put in your email on the page. thanks!',\n",
       " 'Ask HN: How can I effectively narrow down the search space? Especially for complex and ambiguous problems. It&#x27;s difficult not to end up in a rabbit hole. Do you use any heuristics that keep you on track?\\nAny resources that delve on this?',\n",
       " 'Show HN: Neum AI – Open-source large-scale RAG framework Over the last couple months we have been supporting developers in building large-scale RAG pipelines to process millions of pieces of data.<p>We documented our approach in an HN post (<a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37824547\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37824547</a>) a couple weeks ago. Today, we are open sourcing the framework we have developed.<p>The framework focuses on RAG data pipelines and provides scale, reliability, and data synchronization capabilities out of the box.<p>For those newer to RAG, it is a technique to provide context to Large Language Models. It consists of grabbing pieces of information (i.e. pieces of news articles, papers, descriptions, etc.) and incorporating them into prompts to help contextualize the responses. The technique goes one level deeper in finding the right pieces of information to incorporate. The search for relevant information is done through the use of vector embeddings and vector databases.<p>Those pieces of news articles, papers, etc. are transformed into a vector embedding that represents the semantic meaning of the information. These vector representations are organized into indexes where we can quickly search for the pieces of information that most closely resembles (from a semantic perspective) a given question or query. For example, if I take news articles from this year, vectorize them, and add them to an index, I can quickly search for pieces of information about the US elections.<p>To help achieve this, the Neum AI framework features:<p>Starting with built-in data connectors for common data sources, embedding services and vector stores, the framework provides modularity to build data pipelines to your specification.<p>The connectors support pre-processing capabilities to define loading, chunking and selecting strategies to optimize content to be embedded. This also includes extracting metadata that is going to be associated to a given vector.<p>The generated pipelines support large scale jobs through a high throughput distributed architecture. The connectors allow you to parallelize tasks like downloading documents, processing them, generating embedding and ingesting data into the vector DB.<p>For data sources that might be continuously changing, the framework supports data scheduling and synchronization. This includes delta syncs where only new data is pulled.<p>Once data is transformed into a vector database, the framework supports querying of the data including hybrid search using the available metadata added during pre-processing. As part of the querying process, the framework provides capabilities to capture feedback on retrieved data as well as run evaluations against different pipeline configurations.<p>Try it out and if interested in chatting more about this shoot us an email founders@tryneum.com',\n",
       " 'Show HN: A tool built for simple need is helping 1k+ users find their hiddn gems Over the last few years, I got really frustrated with more and more content I saved. I had 3k bookmarks, 500+ on Twitter &#x2F; X, 800+ on LinkedIn and I was never opening any of them again.<p>My default go-to search engine Google still relied on page rank-based results filled with SEO content and ads. I spoke with 50+ users about how they use search, social media, their bookmarks, etc. Insights I learned from this led me to build a simple tool Stacks.<p>It is a search co-pilot tool that provides recommendations using the knowledge that is left hidden and forgotten in bookmarks, research notes, newsletters, and knowledge repositories across the platforms.',\n",
       " 'Show HN: Gosax – A high-performance SAX XML parser for Go I&#x27;ve just released gosax, a new Go library for high-performance SAX (Simple API for XML) parsing. It&#x27;s designed for efficient, memory-conscious XML processing, drawing inspiration from quick-xml and pkg&#x2F;json.\\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;orisano&#x2F;gosax\">https:&#x2F;&#x2F;github.com&#x2F;orisano&#x2F;gosax</a>\\nKey features:<p>- Read-only SAX parsing\\n- Highly efficient parsing using techniques inspired by quick-xml and pkg&#x2F;json\\n- SWAR (SIMD Within A Register) optimizations for fast text processing<p>gosax is particularly useful for processing large XML files or streams without loading the entire document into memory. It&#x27;s well-suited for data feeds, large configuration files, or any scenario where XML parsing speed is crucial.\\nI&#x27;d appreciate any feedback, especially from those working with large-scale XML processing in Go. What are your current pain points with XML parsing? How could gosax potentially help your projects?',\n",
       " 'Ask HN: Have you used a good general purpose LLM-powered scraper? I&#x27;m looking to either build or use a model or SaaS that could be used with a prompt like:<p>Find the 10 biggest realtors in [CITY], then extract all their listings within [BUDGET] in JSON with all the data you can find.<p>I&#x27;m looking for a tool that can use search inputs on any website, click next page buttons or handle infinite scroll.<p>Is there a tool like that on the market that actually delivers?What I&#x27;ve seen for now doesn&#x27;t seem good enough.',\n",
       " 'Show HN: Quick (1 sec) neural network builder with architecture search This project grew out of my frustrations with wanting to experiment with NNs, while a) not knowing which architectures to use, and b) waiting forever for training to finish using standard algorithms.<p>Both tools here are for tabular numeric data.  The quick way to try them out is to check the example button and hit Submit, but feel free to upload your own training data (100 kb limit).  Apologies for the inconsistent page design - I will work on that this week.<p>If anyone has ideas for applications or plugins I should write, I’d love to hear them!',\n",
       " 'Ask HN: Surviving a Mid-Career Crisis I’m nominally a senior engineer with a BS CS. I spent the majority of my career at a FAANG, doing what most people would consider “low-impact” work. No fancy algorithms or large-scale services, just frontend work, mid-level design, and building dev tooling. Generally high performance reviews and I enjoyed mentoring junior programmers and collaborating with stakeholders. I left after the pandemic because I was exhausted and floundering, and my mental health had bottomed out.<p>I’ve spent a few years doing no salaried work other than a &lt;year stint at a company where some serious mismanagement led to me cutting my losses. It’s been a year since I’ve worked, but I’ve kept myself learning with trying new languages, implementing things like transpilers, task schedulers, and tooling for personal monorepos, picking up Rust, playing with coroutines in C++, doing code health for small open source projects, and practicing Leetcode.<p>I’m now in my mid-30s, 15 years into my career. I’ve started my job search and, not even a few weeks in, I’m starting to see the problems:<p>1. My resume has a lot of things on it, but not the sort of impact people seem to want demonstrated (hard numbers like scaling a service to X00,000 users or improving latency on critical services by Xms in the 99th percentile).<p>2. I’ve always preferred to be a generalist, despite the stigma, because I’ve always felt specialization to be a risk in a field with constantly emerging new technologies, and because I’ve never had the temperament or confidence to build sophisticated expert experience on my knowledge base. I’ve never had the confidence to look at a coding exercise and say, “yeah, I can do that”, _despite having a degree in CS and working for a high-profile company for almost a decade_.<p>3. Jobs want experience in e.g. specific cloud services, or JavaScript frameworks, or CI tools; things that would be “third-party” inside my previous FAANG that I have no experience with.<p>4.  The very fact that I’ve taken so much time off, despite working on and executing large personal projects, seems to turn hirers off. I don’t regret the “sabbatical” and I’ve written tens of thousands of lines of code and even projects that needed someone with algorithms experience to pull off, like maximal squares, but that doesn’t interest hiring teams.<p>On top of that:<p>1. The job market is a bloodbath, with layoffs dot fyi claiming 1113 tech companies laid off 249,354 employees in 2023 alone, and 164,969 employees in 2022. Obviously not necessarily all engineers, but you can see how I might fear being inadequate compared with some of the best performers from the best companies.<p>2. I’m terrified about AI becoming the standard for jobs like mine, precisely because I am not an industry-level or subject-matter expert in any specific IT field. I seem to be precisely the person targeted by the use of AI to facilitate software development.<p>Do I drop out of the field entirely? Settle on low-paid consultant work and become another software plumber hoping my clients never decide to just turn to AI instead? Try and tighten up my resume and give recruiters what they want to hear? I’ve always considered the “put everything you’ve ever worked on on your resume” to be a form of deception; I worked on Android development in 2015, but that doesn’t make me an Android programmer or even suggest those skills are useful (pre-Kotlin, pre-who knows how many API levels, etc).<p>EDIT: Also if it helps I&#x27;m in NYC; the job market here is extremely hot and filled with high performers and experts.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_fusion(\"How do I perform a KNN search on a large scale?\", n_docs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a69b489-f3e1-44c0-b07e-7cc969ad4b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/15/25 17:44:56] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> max_tokens is required for Anthropic, setting to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span>                     <a href=\"file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py#279\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">279</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/15/25 17:44:56]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m max_tokens is required for Anthropic, setting to \u001b[1;36m8192\u001b[0m                     \u001b]8;id=863522;file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py\u001b\\\u001b[2mutils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=750958;file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py#279\u001b\\\u001b[2m279\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I can offer limited information about large-scale KNN search.\n",
      "\n",
      "The context mentions a few relevant points:\n",
      "\n",
      "1. **Approximate k-NN algorithms**: One post asks about the state-of-the-art approximate k-NN search algorithms, specifically mentioning IVF, LSH, and HNSW as options, though no definitive answer is provided about which is best.\n",
      "\n",
      "2. **KNN in RAG systems**: The Neum AI framework description mentions using \"kNN search that the agent can use to find the right data in the db\" as part of their RAG (Retrieval-Augmented Generation) system. They describe using vector embeddings and vector databases to search for semantically similar information at scale.\n",
      "\n",
      "3. **Vector embeddings approach**: The context describes transforming content (like news articles, papers, etc.) into vector embeddings that represent semantic meaning, then organizing these into indexes for quick similarity searches.\n",
      "\n",
      "However, I must admit that the provided context doesn't contain comprehensive information about implementing large-scale KNN search systems. The context lacks specific technical details about algorithms, infrastructure considerations, performance optimizations, or concrete implementation approaches for large-scale scenarios. For a complete answer on this topic, you would need additional resources beyond what's provided here.\n"
     ]
    }
   ],
   "source": [
    "response = await rag(\n",
    "    \"How do I perform a KNN search on a large scale?\", \n",
    "    retrieve_func=retrieve_fusion\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb48fc9f-bd1c-4ddb-8dbc-f53d439561d8",
   "metadata": {},
   "source": [
    "More complex problems may require running better rerankers to capture the data nuances. That's why we also created ColBERT embeddings, and it's finally time to test them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d20abccd-e8d4-4c00-a3c4-12911be37b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_colbert_reranking(q: str, n_docs: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve documents based on the provided query\n",
    "    with BM25 and dense retrieval + ColBERT to merge them.\n",
    "    \"\"\"\n",
    "    result = client.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        prefetch=[\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=q,\n",
    "                    model=BM25_MODEL_NAME,\n",
    "                ),\n",
    "                using=BM25_VECTOR_NAME,\n",
    "                limit=n_docs,\n",
    "            ),\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=q,\n",
    "                    model=MODEL_NAME,\n",
    "                ),\n",
    "                using=VECTOR_NAME,\n",
    "                limit=n_docs,\n",
    "            ),\n",
    "        ],\n",
    "        # Reranking with ColBERT embeddings\n",
    "        query=models.Document(\n",
    "            text=q,\n",
    "            model=MUTLIVECTOR_MODEL_NAME,\n",
    "        ),\n",
    "        using=MULTIVECTOR_NAME,\n",
    "        limit=n_docs,\n",
    "    )\n",
    "    docs = [\n",
    "        f\"{point.payload['title']} {point.payload['text']}\"\n",
    "        for point in result.points\n",
    "    ]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e0c4cd6-4fe6-4595-9e6b-50ced9ddac17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[09/15/25 17:46:45] </span><span style=\"color: #808000; text-decoration-color: #808000\">WARNING </span> max_tokens is required for Anthropic, setting to <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span>                     <a href=\"file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py#279\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">279</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[09/15/25 17:46:45]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m max_tokens is required for Anthropic, setting to \u001b[1;36m8192\u001b[0m                     \u001b]8;id=351931;file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py\u001b\\\u001b[2mutils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=73516;file:///home/kacper/Projects/Qdrant/workshop-improving-r-in-rag/.venv/lib/python3.12/site-packages/any_llm/providers/anthropic/utils.py#279\u001b\\\u001b[2m279\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I can only offer limited information about large-scale k-NN (k-nearest neighbor) search.\n",
      "\n",
      "From the context, there is a question asking \"What is the state of art approximate k-NN search algorithm today?\" and mentions that someone is interested in learning what kind of ANN (Approximate Nearest Neighbor) algorithms big search companies like Google use, specifically mentioning IVF, LSH, and HNSW as examples.\n",
      "\n",
      "Additionally, there's a mention in the Neum AI framework description of using \"kNN search\" with metadata embeddings to help an AI agent find the right data in databases, and reference to vector embeddings being used for semantic search where \"vector representations are organized into indexes where we can quickly search for the pieces of information that most closely resembles (from a semantic perspective) a given question or query.\"\n",
      "\n",
      "However, the context doesn't provide specific implementation details, performance characteristics, or concrete guidance on how to actually perform large-scale k-NN search. The context raises the question about state-of-the-art approaches but doesn't answer it with technical specifics.\n",
      "\n",
      "I cannot provide a comprehensive answer about performing large-scale k-NN search based solely on this context, as it lacks the detailed technical information needed to properly address your question.\n"
     ]
    }
   ],
   "source": [
    "response = await rag(\n",
    "    \"How do I perform a KNN search on a large scale?\", \n",
    "    retrieve_func=retrieve_colbert_reranking\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c8cb304-231a-44f0-a182-5a2d8f416e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Show HN: Neum AI – Open-source large-scale RAG framework Over the last couple months we have been supporting developers in building large-scale RAG pipelines to process millions of pieces of data.<p>We documented our approach in an HN post (<a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37824547\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37824547</a>) a couple weeks ago. Today, we are open sourcing the framework we have developed.<p>The framework focuses on RAG data pipelines and provides scale, reliability, and data synchronization capabilities out of the box.<p>For those newer to RAG, it is a technique to provide context to Large Language Models. It consists of grabbing pieces of information (i.e. pieces of news articles, papers, descriptions, etc.) and incorporating them into prompts to help contextualize the responses. The technique goes one level deeper in finding the right pieces of information to incorporate. The search for relevant information is done through the use of vector embeddings and vector databases.<p>Those pieces of news articles, papers, etc. are transformed into a vector embedding that represents the semantic meaning of the information. These vector representations are organized into indexes where we can quickly search for the pieces of information that most closely resembles (from a semantic perspective) a given question or query. For example, if I take news articles from this year, vectorize them, and add them to an index, I can quickly search for pieces of information about the US elections.<p>To help achieve this, the Neum AI framework features:<p>Starting with built-in data connectors for common data sources, embedding services and vector stores, the framework provides modularity to build data pipelines to your specification.<p>The connectors support pre-processing capabilities to define loading, chunking and selecting strategies to optimize content to be embedded. This also includes extracting metadata that is going to be associated to a given vector.<p>The generated pipelines support large scale jobs through a high throughput distributed architecture. The connectors allow you to parallelize tasks like downloading documents, processing them, generating embedding and ingesting data into the vector DB.<p>For data sources that might be continuously changing, the framework supports data scheduling and synchronization. This includes delta syncs where only new data is pulled.<p>Once data is transformed into a vector database, the framework supports querying of the data including hybrid search using the available metadata added during pre-processing. As part of the querying process, the framework provides capabilities to capture feedback on retrieved data as well as run evaluations against different pipeline configurations.<p>Try it out and if interested in chatting more about this shoot us an email founders@tryneum.com',\n",
       " 'Show HN: ChatGPT for Your Data I built an ai agent that can connect to your database.\\nThere&#x27;s a lot of text-to-sql ai tools out there, here are the 2 key differences about my approach:\\n1. AI Agent - I give the Agent a bunch of tools and let it decide what tools to call, in what order, with which parameters, in order to help the user. this means you can ask super vague questions and the agent helps you think through coming up with a good answer.\\n2. Metadata graph &amp; embeddings - I use an LLM to create metadata about the db schemas including things like join keys, column descriptions, and table contents. I also convert this metadata into embeddings. this powers a kNN search that the agent can use to find the right data in the db.<p>if you would be willing to give it a try please put in your email on the page. thanks!',\n",
       " 'Show HN: GritQL, a Rust CLI for rewriting source code Hi everyone!<p>I’m excited to open source GritQL, a Rust CLI for searching and transforming source code.<p>GritQL comes from my experiences with conducting large scale refactors and migrations.<p>Usually, I would start exploring a codebase with grep. This is easy to start with, but most migrations end up accumulating additional requirements like ensuring the right packages are imported and excluding cases which don’t have a viable migration path.<p>Eventually, to build a complex migration, I usually ended up having to write a full codemod program with a tool like jscodeshift. This comes with its own problems:<p>- Most of the exploratory work has to be abandoned as you figure out how to represent your original regex search as an AST.\\n- Reading&#x2F;writing a codemod requires mentally translating from AST names back to what source code actually looks like.\\n- Performance is often an afterthought, so iterating on a large codemod can be painfully slow.\\n- Codemod frameworks are language-specific, so if you’re hopping between multiple languages—or trying to migrate a shared API—you have to learn different tools.<p>GritQL is an attempt to develop a powerful middle ground:\\n- Exploratory analysis is easy: just put a code snippet in backticks and use $metavariables for placeholders.\\n- Incrementally add complexity by introducing side conditions with where clauses.\\n- Reuse named patterns to avoid rebuilding queries, and use shared patterns from our standard library for common tasks like ensuring modules are imported.\\n- Iterate on large codebases quickly: we use Rust for maximum performance<p>GritQL has already been used on thousands of repositories for complex migrations[1] but we&#x27;re excited to collaborate more with the open source community.<p>[1] Ex. <a href=\"https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;openai-python&#x2F;discussions&#x2F;742\">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;openai-python&#x2F;discussions&#x2F;742</a>',\n",
       " 'Ask HN: Supabase vs. Neon Database: Which One Should I Choose? I wonder if HN could share their favorite serverless Postgres database?<p>I&#x27;m currently evaluating different database solutions for a SaaS I&#x27;m working on, and I&#x27;ve narrowed it down to Supabase and Neon Database. I&#x27;ve done some initial research, but I&#x27;m looking for more insights from the community.<p>Here are a few specifics about my project:<p>* Scale: Anticipating moderate to high traffic\\n* Tech Stack: very minimal I can go for a web app:- JavaScript, TypeScript using NextJS, ReactJS- TailwindCSS, RadixUI for UI- Deploy on Vercel \\n* Key Requirements: Real-time capabilities, ease of use, performance, cost-efficiency, and good support&#x2F;docs.<p>Questions:<p>1. Performance and Scalability: How do these two compare in terms of handling large-scale applications? Any real-world experiences would be great.\\n2. Ease of Use: Which one has a more developer-friendly interface and better docs?\\n3. Real-time Features: How do their real-time features stack up against each other?\\n4. Community and Support: How active and helpful are their communities and support teams?\\n5. Cost: Any insights on the cost-effectiveness of each, especially for scaling applications?<p>I&#x27;m leaning towards one but would love to hear your thoughts and experiences.',\n",
       " 'Ask HN: What is the state of art approximate k-NN search algorithm today? For someone interested in learning more about search, what kind of ANN does big search company like google use?<p>IVF, LSH or even HNSW?',\n",
       " 'Bark – An Open Source tool for sub-terabyte scale log aggregation So I, along with others mates from the Open Source community created a log aggregation tool (backend tool) named Bark ( https:&#x2F;&#x2F;github.com&#x2F;techrail&#x2F;bark ). Some of you might be rolling your eyes thinking &quot;yet another log aggregation tool in the pool of already existing dozens over dozens of log aggregators...why does anyone build these things!?&quot; So here are the salient points about it:<p>1. It contains an aggregator server and a client - both mainly written in go but Java client is being worked upon.<p>2. It tries to be very easy to setup and use - binaries and docker images are available. Supply the DB URL which contains the required schema and start the server and that&#x27;s it. The client library is even easier to use (a little on that later).<p>3. It sends the logs to a PostgreSQL server - Since PostgerSQL is much more familiar to many developers than any other search engine (like Lucene&#x2F;ELK) and easier to setup than a large APM solution, it is fairly straightforward for most (of course, not all) backend developers.<p>Now, on paper this looks like a really bad decision since PostgreSQL is not suitable for crazy insert speeds or loads of textual data or FTS. However, the focus here is to be like a step between plaintext logs and the enterprise-ready terabyte-scale software like ELK, NewRelic, Splunk, DataDog etc. This tool (bark) does not aim to be an APM at all and is not really targeted towards installations which produce more than a few GBs of data everyday. For such usecases, it is better to use other tools. But it does try to ease the way into structured remote logging. The performance of the server is approximately around 2000-3000 logs per second without an index and 1500-2000 with a single-column index on timestamp column and 5 DB connections.<p>The go client supports 3 modes of operation:<p>1. stdout logging: The most normal case - you just start dumping the logs to your standard output. You can also choose to dump it in a file. It uses the `log&#x2F;slog` package from go 1.21 for achieving this.<p>2. Sending logs to a server: You can create a client of bark which can connect to Bark server which can accept REST API calls and save those entries to the DB. This is useful when you have multiple services which want to store the logs at a central place. You can still do stdout logging on the client.<p>3. Embedded server side in the client: This use-case is in between the two options above. The idea is that when you still have a monolith but want to send the logs to the DB (you can still dump a copy of the logs to your stdout), but you don&#x27;t want to setup another service.<p>The interesting point is - you can use the embedded server side in client method at scale too: you can just create a client in all (multiple) services which then send the logs directly to the DB.<p>I hope it helps someone other than me too.',\n",
       " 'Ask HN: Surviving a Mid-Career Crisis I’m nominally a senior engineer with a BS CS. I spent the majority of my career at a FAANG, doing what most people would consider “low-impact” work. No fancy algorithms or large-scale services, just frontend work, mid-level design, and building dev tooling. Generally high performance reviews and I enjoyed mentoring junior programmers and collaborating with stakeholders. I left after the pandemic because I was exhausted and floundering, and my mental health had bottomed out.<p>I’ve spent a few years doing no salaried work other than a &lt;year stint at a company where some serious mismanagement led to me cutting my losses. It’s been a year since I’ve worked, but I’ve kept myself learning with trying new languages, implementing things like transpilers, task schedulers, and tooling for personal monorepos, picking up Rust, playing with coroutines in C++, doing code health for small open source projects, and practicing Leetcode.<p>I’m now in my mid-30s, 15 years into my career. I’ve started my job search and, not even a few weeks in, I’m starting to see the problems:<p>1. My resume has a lot of things on it, but not the sort of impact people seem to want demonstrated (hard numbers like scaling a service to X00,000 users or improving latency on critical services by Xms in the 99th percentile).<p>2. I’ve always preferred to be a generalist, despite the stigma, because I’ve always felt specialization to be a risk in a field with constantly emerging new technologies, and because I’ve never had the temperament or confidence to build sophisticated expert experience on my knowledge base. I’ve never had the confidence to look at a coding exercise and say, “yeah, I can do that”, _despite having a degree in CS and working for a high-profile company for almost a decade_.<p>3. Jobs want experience in e.g. specific cloud services, or JavaScript frameworks, or CI tools; things that would be “third-party” inside my previous FAANG that I have no experience with.<p>4.  The very fact that I’ve taken so much time off, despite working on and executing large personal projects, seems to turn hirers off. I don’t regret the “sabbatical” and I’ve written tens of thousands of lines of code and even projects that needed someone with algorithms experience to pull off, like maximal squares, but that doesn’t interest hiring teams.<p>On top of that:<p>1. The job market is a bloodbath, with layoffs dot fyi claiming 1113 tech companies laid off 249,354 employees in 2023 alone, and 164,969 employees in 2022. Obviously not necessarily all engineers, but you can see how I might fear being inadequate compared with some of the best performers from the best companies.<p>2. I’m terrified about AI becoming the standard for jobs like mine, precisely because I am not an industry-level or subject-matter expert in any specific IT field. I seem to be precisely the person targeted by the use of AI to facilitate software development.<p>Do I drop out of the field entirely? Settle on low-paid consultant work and become another software plumber hoping my clients never decide to just turn to AI instead? Try and tighten up my resume and give recruiters what they want to hear? I’ve always considered the “put everything you’ve ever worked on on your resume” to be a form of deception; I worked on Android development in 2015, but that doesn’t make me an Android programmer or even suggest those skills are useful (pre-Kotlin, pre-who knows how many API levels, etc).<p>EDIT: Also if it helps I&#x27;m in NYC; the job market here is extremely hot and filled with high performers and experts.',\n",
       " 'Ask HN: Have you used a good general purpose LLM-powered scraper? I&#x27;m looking to either build or use a model or SaaS that could be used with a prompt like:<p>Find the 10 biggest realtors in [CITY], then extract all their listings within [BUDGET] in JSON with all the data you can find.<p>I&#x27;m looking for a tool that can use search inputs on any website, click next page buttons or handle infinite scroll.<p>Is there a tool like that on the market that actually delivers?What I&#x27;ve seen for now doesn&#x27;t seem good enough.',\n",
       " 'Show HN: We\\'re building an open data warehouse inspired by Git scraping Hey everyone, this is Jason and Nathan from <a href=\"https:&#x2F;&#x2F;subsets.io\" rel=\"nofollow\">https:&#x2F;&#x2F;subsets.io</a>, a new open data warehouse. Our goal is to make finding and accessing public data easier for human analysis, in apps, or as a source of up-to-date data for retrieval-augmented-generation.<p>Inspired by git scraping [1], the core idea is to build something where people don’t upload a snapshot of their dataset directly, like you might do on Kaggle or Huggingface. Instead, anyone can contribute code (connectors) which we then continuously run and make the fetched data available for everyone in our shared, public data warehouse. We currently have connectors for 120+ datasets including an index of YC companies, U.S. house prices, and Wikipedia search volumes.<p>Separately, open data portals, such as from NGOs, can be hard to use due to their use of semantic web principles - i.e., representing data as a graph and adding structured metadata. We’re taking a less structured approach: each dataset is just a table that you can download or query using SQL, and we’re building a machine learning engine for ranking, pre-processing, and to generate relevant subsets&#x2F;views from the data warehouse.<p>BigQuery is used as the data warehouse. We use dagster for the data pipelines, running it on top of Kubernetes. Frontend is NextJS. The data pipelines are currently centralised in our repo, but we’re building our own engine where you can just upload simple scripts. Search is currently basic semantic search, with one big index that stores unique strings across tables, columns, and rows. Before we used better search using LLM’s, but the cost, latency, and rate limits mean we’re still investigating the right way to go.<p>The project is in its very beginning stages, but we’d like to get some early feedback and find people who either want to help us build connectors or use the data to build something cool. The connectors are available at <a href=\"https:&#x2F;&#x2F;github.com&#x2F;subsetsio&#x2F;subsets-connectors\">https:&#x2F;&#x2F;github.com&#x2F;subsetsio&#x2F;subsets-connectors</a>, and you can visually explore the datasets and get your own free API key at <a href=\"https:&#x2F;&#x2F;www.subsets.io\" rel=\"nofollow\">https:&#x2F;&#x2F;www.subsets.io</a>.<p>[1] - <a href=\"https:&#x2F;&#x2F;simonwillison.net&#x2F;2020&#x2F;Oct&#x2F;9&#x2F;git-scraping&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;simonwillison.net&#x2F;2020&#x2F;Oct&#x2F;9&#x2F;git-scraping&#x2F;</a>',\n",
       " 'Show HN: Grafbase – Introducing Federated Graphs Hi, Kate here! Grafbase is excited to announce the introduction of Federated Graphs, a significant enhancement to our GraphQL services. This update marks a pivotal shift from our traditional front-end focus to a broader, more enterprise-scale approach. Here&#x27;s everything you need to know about this new feature launch.<p>Federation allows each independent subgraph to be stitched together into a unified graph that can be exposed to end users. The Federation gateway determines the most efficient query plan for every query, optimizing for resource use.<p>Additionally, Grafbase Federated Graphs have:<p>- Support for Federation V2 Spec: Aligned with the latest Federation specifications, our Federated Graphs ensure compatibility and performance at the highest level.\\n- High-Performance: Crafted in Rust, Federated Graphs offer exceptional performance and reliability, crucial for large-scale applications.\\n- Open Source and SDK Integration: Open source availability fosters community engagement and collaboration, while our SDK ensures easy deployment to Grafbase.\\n- Advanced Security and Efficiency: Robust authentication and edge caching mechanisms enhance both the security and efficiency of Federated Graphs.\\n- Full Stack Deployment on Grafbase: Grafbase supports the deployment of both Federated Graphs and Standalone Subgraphs, providing a comprehensive platform for all GraphQL needs.<p>For now, I&#x27;m just excited to share what Grafabse has been working on with the HN community. Would love to know what you guys think!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_colbert_reranking(\"How do I perform a KNN search on a large scale?\", n_docs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63535bc-722d-4f13-a4ac-08eadead3cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
